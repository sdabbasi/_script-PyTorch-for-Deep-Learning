{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b643b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ann_files/NYCTaxiFares.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353931c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fare_amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497b344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a21928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the longitude and latitude values change slightly through the travels, it is better to use kind of distance instead\n",
    "# of them (this process is called feature engineering)\n",
    "def haversine_distance(df, lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat2]-df[lat1])\n",
    "    delta_lambda = np.radians(df[long2]-df[long1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c) # in kilometers\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dist_km'] = haversine_distance(df, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eecfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ac4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the pickup_datetime:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime is a non-sense feature currently for the network and we should do some feature engineering on that too !!!\n",
    "# first: change the currently defined pickup_datetime which is string to datetime (to become more understandable)\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7146189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second: change the UTC time to real New York timing which is Eastern Datetime\n",
    "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f52ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third: extract some categorical data\n",
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour']<12, 'am', 'pm')\n",
    "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8da523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c543fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all features are prepared; some are categorical and some are continuous: let's handle them:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Hour', 'AMorPM', 'Weekday']\n",
    "cont_cols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km']\n",
    "y_col = ['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaba05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06dee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    df[cat] = df[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716dc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3052f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb95d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91dbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d784ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7eac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = df['Hour'].cat.codes.values\n",
    "am_pm = df['AMorPM'].cat.codes.values\n",
    "week_day = df['Weekday'].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.stack([hr, am_pm, week_day], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "# here we used a one-line code structure; we could use this for 'cats' variable too, but we used separate lines to be more understandable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = torch.tensor(cats, dtype=torch.int64)\n",
    "conts = torch.tensor(conts, dtype=torch.float)\n",
    "y = torch.tensor(df[y_col].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c408c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to define the embedding specifications for the categorical data\n",
    "# The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50.\n",
    "cat_szs = [len(df[col].cat.categories) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45dc5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccb705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## the following lines would just illustrate that how does embedding work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc539ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cat = cats[:2]\n",
    "sub_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_embeds = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for num_embeddings, embedding_dim in emb_szs])\n",
    "self_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df842b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingz = []\n",
    "for i, e in enumerate(self_embeds):\n",
    "    embeddingz.append(e(sub_cat[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_embeddings = torch.cat(embeddingz, axis=1)\n",
    "concated_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_drops = nn.Dropout(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_embeddings_dropout = self_drops(concated_embeddings)\n",
    "concated_embeddings_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65687f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b66e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for num_embeddings, embedding_dim in emb_szs])\n",
    "        self.embed_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerslist = []\n",
    "        num_embeds = sum([embedding_dim for num_embeddings, embedding_dim in emb_szs])\n",
    "        num_in = num_embeds + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerslist.append(nn.Linear(num_in, i))\n",
    "            layerslist.append(nn.ReLU(inplace=True))\n",
    "            layerslist.append(nn.BatchNorm1d(i))\n",
    "            layerslist.append(nn.Dropout(p))\n",
    "            num_in = i\n",
    "            \n",
    "        layerslist.append(nn.Linear(layers[-1],out_sz))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layerslist)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x_cat = torch.cat(embeddings, axis=1)\n",
    "        x_cat = self.embed_drop(x_cat)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        \n",
    "        x = torch.cat([x_cat, x_cont], axis=1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, conts.shape[1], 1, [200, 100], p=0.4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define a sort of nominal batch size to just cut in half the training time. Pay attention that batch size here is just\n",
    "# nominal and is not the technical usage of batch size which leads to batch gradient descent\n",
    "batch_size = 60000\n",
    "test_size = int(batch_size*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IS SHUFFLED ALREADY\n",
    "cat_train = cats[:batch_size-test_size]\n",
    "cat_test = cats[batch_size-test_size:batch_size]\n",
    "cont_train = conts[:batch_size-test_size]\n",
    "cont_test = conts[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:batch_size-test_size]\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epochs = 300\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(cat_train, cont_train)\n",
    "    loss = torch.sqrt(criterion(y_pred, y_train))     # this means RMSE\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i%10 ==1:\n",
    "        print(f'epoch: {i}, loss is {loss}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "duration = time.time() - start_time\n",
    "print(f'training took {duration/60} minutes')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543629f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model(cat_test, cont_test)\n",
    "    loss = torch.sqrt(criterion(y_val, y_test))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684cbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    diff = np.abs(y_val[i].item() - y_test[i].item())\n",
    "    print(f'{i}) predicted: {y_val[i].item(): 8.2f},      true: {y_test[i].item(): 8.2f}    diff: {diff: 8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/my_taxi_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68aead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
