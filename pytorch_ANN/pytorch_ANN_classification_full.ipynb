{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ann_files/NYCTaxiFares.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fare_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f784881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the longitude and latitude values change slightly through the travels, it is better to use kind of distance instead\n",
    "# of them (this process is called feature engineering)\n",
    "def haversine_distance(df, lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat2]-df[lat1])\n",
    "    delta_lambda = np.radians(df[long2]-df[long1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c) # in kilometers\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dist_km'] = haversine_distance(df, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79555a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the pickup_datetime:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafe7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime is a non-sense feature currently for the network and we should do some feature engineering on that too !!!\n",
    "# first: change the currently defined pickup_datetime which is string to datetime (to become more understandable)\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f51083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second: change the UTC time to real New York timing which is Eastern Datetime\n",
    "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf091795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third: extract some categorical data\n",
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour']<12, 'am', 'pm')\n",
    "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all features are prepared; some are categorical and some are continuous: let's handle them:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Hour', 'AMorPM', 'Weekday']\n",
    "cont_cols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km']\n",
    "y_col = ['fare_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    df[cat] = df[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b703c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520da1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed191d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04910116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e14d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a12a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50378d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = df['Hour'].cat.codes.values\n",
    "am_pm = df['AMorPM'].cat.codes.values\n",
    "week_day = df['Weekday'].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7affcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.stack([hr, am_pm, week_day], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e54fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "# here we used a one-line code structure; we could use this for 'cats' variable too, but we used separate lines to be more understandable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = torch.tensor(cats, dtype=torch.int64)\n",
    "conts = torch.tensor(conts, dtype=torch.float)\n",
    "y = torch.tensor(df[y_col].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ecc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4e6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a628a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to define the embedding specifications for the categorical data\n",
    "# The rule of thumb for determining the embedding size is to divide the number of unique entries in each column by 2, but not to exceed 50.\n",
    "cat_szs = [len(df[col].cat.categories) for col in cat_cols]\n",
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5bb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b90eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for num_embeddings, embedding_dim in emb_szs])\n",
    "        self.embed_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerslist = []\n",
    "        num_embeds = sum([embedding_dim for num_embeddings, embedding_dim in emb_szs])\n",
    "        num_in = num_embeds + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerslist.append(nn.Linear(num_in, i))\n",
    "            layerslist.append(nn.ReLU(inplace=True))\n",
    "            layerslist.append(nn.BatchNorm1d(i))\n",
    "            layerslist.append(nn.Dropout(p))\n",
    "            num_in = i\n",
    "            \n",
    "        layerslist.append(nn.Linear(layers[-1],out_sz))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layerslist)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x_cat = torch.cat(embeddings, axis=1)\n",
    "        x_cat = self.embed_drop(x_cat)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        \n",
    "        x = torch.cat([x_cat, x_cont], axis=1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56452c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, conts.shape[1], 2, [200, 100], p=0.4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6347048",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02779c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define a sort of nominal batch size to just cut in half the training time. Pay attention that batch size here is just\n",
    "# nominal and is not the technical usage of batch size which leads to batch gradient descent\n",
    "batch_size = 60000\n",
    "test_size = int(batch_size*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b498305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IS SHUFFLED ALREADY\n",
    "cat_train = cats[:batch_size-test_size]\n",
    "cat_test = cats[batch_size-test_size:batch_size]\n",
    "cont_train = conts[:batch_size-test_size]\n",
    "cont_test = conts[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:batch_size-test_size]\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epochs = 300\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(cat_train, cont_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i%10 ==1:\n",
    "        print(f'epoch: {i}, loss is {loss}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "duration = time.time() - start_time\n",
    "print(f'training took {duration/60} minutes')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed998cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model(cat_test, cont_test)\n",
    "    loss = criterion(y_val, y_test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ea3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'{i}) raw: {y_val[i]}    predicted: {y_val[i].argmax().item(): 8.2f},      true: {y_test[i]: 8.2f}    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/my_taxi_model_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991a7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('myMLEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "418a5ed4ed6d8198b840b61453495c26d507d70d3841dd181a7388aaa4474a5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
